{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b439967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d59f806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # 應該為 True\n",
    "print(torch.version.cuda)         # 應該列出 CUDA 版本\n",
    "print(torch.backends.cudnn.version())  # cuDNN 版本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9d18a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.path.join(os.getcwd(), 'space_ship_game_RL')\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "from setting import *\n",
    "from game import Game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa1809ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceShipEnv():\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        pygame.font.init()\n",
    "\n",
    "        # 延後畫面初始化，等 render() 時才設置\n",
    "        self.screen = None\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.fps = FPS\n",
    "\n",
    "        self.game = Game()\n",
    "\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.observation = self.game.state\n",
    "        self.prev_score = 0\n",
    "        self.prev_health = self.game.player.sprite.health\n",
    "\n",
    "    def step(self, action):\n",
    "        self.game.update(action)\n",
    "\n",
    "        if self.screen is None:\n",
    "            self.game.draw()\n",
    "        else:\n",
    "            self.game.draw(self.screen)\n",
    "            self.clock.tick(self.fps)\n",
    "\n",
    "        # define the state by your game logic\n",
    "        state = self.game.state\n",
    "\n",
    "        # define the reward by your game logic\n",
    "        reward = 0.0\n",
    "\n",
    "        # 擊中石頭\n",
    "        if self.game.is_hit_rock:\n",
    "            hit_inc = self.game.score - self.prev_score      # 新增分數差\n",
    "            # rock.radius 已映射進分數增量，可直接用\n",
    "            reward +=  1.5*hit_inc                     # 相當於 k1≈0.2*2 = 0.4\n",
    "\n",
    "        # 撿到護盾 / 武器\n",
    "        if self.game.is_power_shield:\n",
    "            reward += 5\n",
    "        if self.game.is_power_gun:\n",
    "            reward += 3\n",
    "\n",
    "        # 撞到石頭（血量已在 game.collide_player_rock 調整）\n",
    "        if self.game.is_collided:\n",
    "            damage = self.game.is_collided_radius   # 你可在 collide_player_rock 回傳 radius\n",
    "            reward -= 15 * (damage / 10)\n",
    "\n",
    "        # 存活獎勵\n",
    "        reward += 0.2\n",
    "\n",
    "        # 死亡懲罰（再降低）\n",
    "        if not self.game.running:\n",
    "            reward -= 200\n",
    "        # 更新上一步的狀態\n",
    "        self.prev_score = self.game.score\n",
    "        self.prev_health = self.game.player.sprite.health\n",
    "        # ------------------------------------\n",
    "    \n",
    "        done = not self.game.running or self.game.score >= 10000\n",
    "\n",
    "        info = {\n",
    "            \"score\": self.game.score,\n",
    "            \"health\": self.game.player.sprite.health\n",
    "        }\n",
    "\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.game = Game()\n",
    "        self.prev_score = 0\n",
    "        return self.game.state\n",
    "\n",
    "    def render(self):\n",
    "        if self.screen is None:\n",
    "            self.screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "            pygame.display.set_caption(\"SpaceShip RL Environment\")\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7476598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 4000\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "lr = 1e-4\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.999\n",
    "memory_capacity = 100000  # 100000\n",
    "target_update_freq = 2000   # 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd292474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15d4b6bf970>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_list=[]\n",
    "epsilon=epsilon_start\n",
    "for i in range(num_episodes):\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    epsilon_list.append(epsilon)\n",
    "plt.plot(epsilon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf358210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c6925e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        return value + advantage - advantage.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2b584f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNN-based DQN Model\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, in_channels, num_actions):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),  # (C,84,84)→(32,20,20)\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=4, stride=2),           # →(64,9,9)\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),           # →(64,7,7)\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(64 * 7 * 7, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, num_actions)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x / 255.0) \n",
    "#         return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63e271c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "# 用於儲存代理人經驗（狀態、動作、獎勵等）的緩衝區，支援隨機抽樣以打破時間相關性，有助於穩定訓練。\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        # Use deque with a fixed capacity to automatically discard the oldest experience when full.\n",
    "        # 使用 deque 並設定最大長度，當容量滿時會自動移除最舊的資料。\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        # Store a single transition (experience) in the buffer.\n",
    "        # 儲存一筆經驗（狀態轉移）進緩衝區。\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        # Randomly sample a batch of transitions to break correlation between consecutive samples.\n",
    "        # 隨機抽取一批經驗，打破樣本間的時間關聯性，提高訓練穩定性。\n",
    "\n",
    "        # Unpack each element into separate tensors for network input\n",
    "        # 將 batch 拆解成分別的 tensor 以供神經網路訓練\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert the sampled data into tensors and move them to the specified device (CPU or GPU)\n",
    "        # 將抽樣資料轉成 tensor 並移至指定設備（CPU/GPU）\n",
    "\n",
    "        # states = torch.tensor(np.stack(states), dtype=torch.float32, device=device)\n",
    "        # actions = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "        # rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        # next_states = torch.tensor(np.stack(next_states), dtype=torch.float32, device=device)\n",
    "        # dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "        # return states, actions, rewards, next_states, dones\n",
    "        # 檢查 state shape\n",
    "        \n",
    "        # 先在 CPU 上創建 Tensor，然後 pin memory \n",
    "        states = torch.stack(states).pin_memory()\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).pin_memory()\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).pin_memory()\n",
    "        next_states = torch.stack(next_states).pin_memory()\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).pin_memory()\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "        # return (\n",
    "        #     torch.stack(states),\n",
    "        #     torch.tensor(actions, dtype=torch.int64, device=device).unsqueeze(1),  # Add dimension for actions\n",
    "        #     torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1),  # Add dimension for rewards    \n",
    "        #     torch.stack(next_states),\n",
    "        #     torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)  # Add dimension for dones\n",
    "        # )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "        # Return the current size of the buffer.\n",
    "        # 回傳緩衝區目前儲存的資料數量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30848ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess frames (grayscale and resize to 84x84)\n",
    "# 預處理影格：轉為灰階並縮放為 84x84\n",
    "\n",
    "import cv2\n",
    "def preprocess_state(state, stacked_state=None, is_new=False):\n",
    "    gray = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
    "    gray = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    gray = torch.tensor(gray, dtype=torch.uint8).unsqueeze(0)        # (1,84,84)\n",
    "    if is_new or stacked_state is None:\n",
    "        stacked_state = gray.repeat(4, 1, 1)                         # (4,84,84)\n",
    "    else:\n",
    "        stacked_state = torch.cat((gray, stacked_state[:-1]), dim=0)\n",
    "    return stacked_state.float().unsqueeze(0), stacked_state         # (1,4,84,84)\n",
    "\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    # frame 是 numpy array (H, W, 3)，先轉為 PIL Image\n",
    "    # Input is a color image (RGB), convert to PIL format for easier processing.\n",
    "    # 輸入是彩色圖像（RGB），轉成 PIL Image 以方便處理。\n",
    "    image = Image.fromarray(frame)\n",
    "\n",
    "    # 轉灰階\n",
    "    # Convert the image to grayscale to reduce input complexity.\n",
    "    # 將影像轉為灰階，降低輸入維度與計算量。\n",
    "    image = image.convert('L')\n",
    "\n",
    "    # resize 成 84x84\n",
    "    # Resize the image to a standard 84x84 shape, as per DQN convention.\n",
    "    # 依照 DQN 的慣例將影像統一縮放至 84x84。\n",
    "    image = image.resize((84, 84), Image.Resampling.BILINEAR)  # or NEAREST, or LANCZOS\n",
    "\n",
    "    # 轉回 numpy 並正規化\n",
    "    # Convert back to NumPy and normalize pixel values to [0, 1].\n",
    "    # 轉回 NumPy 格式並將像素值標準化到 [0, 1]。\n",
    "    frame = np.asarray(image, dtype=np.float32) / 255.0\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # 預處理目前影格\n",
    "    frame = preprocess_frame(state)\n",
    "\n",
    "    if is_new_episode or stacked_frames is None:\n",
    "        # If it's a new episode or no previous frames, initialize with 4 identical frames\n",
    "        # 若是新的一集或是尚未初始化，則用目前影格複製 4 次形成初始堆疊\n",
    "        stacked_frames = deque([frame]*4, maxlen=4)\n",
    "    else:\n",
    "        # 否則把新影格加入到堆疊中，自動捨棄最舊的\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "    # Stack the 4 frames along the first dimension: shape becomes (4, 84, 84)\n",
    "    # 沿著第一維（channel）堆疊成 4 通道輸入：形狀變成 (4, 84, 84)\n",
    "    stacked_state = np.stack(stacked_frames, axis=0)\n",
    "\n",
    "    return stacked_state, stacked_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5cf42e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (86016x84 and 4x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 94\u001b[0m\n\u001b[0;32m     90\u001b[0m dones \u001b[38;5;241m=\u001b[39m dones\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# 計算 Q 值 / Compute Q values\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43monline_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)  \u001b[38;5;66;03m# 取得當前動作的 Q 值\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# 計算目標 Q 值 / Compute target Q values\u001b[39;00m\n",
      "File \u001b[1;32mz:\\RL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mz:\\RL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 22\u001b[0m, in \u001b[0;36mDuelingDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_stream(x)\n\u001b[0;32m     24\u001b[0m     advantage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantage_stream(x)\n",
      "File \u001b[1;32mz:\\RL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mz:\\RL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mz:\\RL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mz:\\RL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mz:\\RL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mz:\\RL\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (86016x84 and 4x128)"
     ]
    }
   ],
   "source": [
    "# 初始化 Breakout 環境 / Initialize the Breakout environment\n",
    "import torch.amp\n",
    "\n",
    "env = SpaceShipEnv()\n",
    "num_actions = len(env.action_space)\n",
    "state_dim = 4\n",
    "# 建立策略網路與目標網路 / Create policy and target networks\n",
    "online_net = DuelingDQN(state_dim, num_actions).to(device)\n",
    "target_net = DuelingDQN(state_dim, num_actions).to(device)\n",
    "# 並定期同步\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(online_net.parameters(), lr=lr)\n",
    "memory = ReplayMemory(memory_capacity)\n",
    "scaler = torch.amp.GradScaler() \n",
    "\n",
    "# 載入模型（若有）\n",
    "if os.path.exists('checkpoint_V10.pth'):\n",
    "    checkpoint = torch.load('checkpoint_V10.pth', map_location=device)\n",
    "    online_net.load_state_dict(checkpoint['online_net'])\n",
    "    target_net.load_state_dict(checkpoint['target_net'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    start_episode   = checkpoint['episode'] + 1\n",
    "    frame_idx       = checkpoint['total_steps']\n",
    "    best_score      = checkpoint['best_score']\n",
    "    epsilon         = checkpoint['epsilon']\n",
    "    reward_history  = checkpoint['reward_history']\n",
    "    score_history   = checkpoint['score_history']\n",
    "    print(f\"Loaded checkpoint from episode {start_episode}, best score: {best_score:.2f}, epsilon={epsilon:.3f}\")\n",
    "else:\n",
    "    start_episode = 0\n",
    "    frame_idx    = 0\n",
    "    best_score = float('-inf')\n",
    "    epsilon = epsilon_start\n",
    "    reward_history = []\n",
    "    score_history = []\n",
    "\n",
    "\n",
    "# 訓練迴圈 / Training loop\n",
    "# 每次可以訓練個100步(根據自己的設備與時間調整)，然後下一次都再接續上次的訓練模型接續往下訓練\n",
    "# You can train for 100 steps each time (adjust according to your own equipment and time), \n",
    "# and then continue training from the last training model next time.\n",
    "for episode in range(start_episode, 4000): # last time = 3200\n",
    "    cur_state_b, cur_state = preprocess_state(env.reset(), None, True)  # 回傳 4×84×84\n",
    "    episode_reward = 0\n",
    "    # state, stacked_frames = stack_frames(None, state, True)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # ε-greedy 策略：隨機選擇動作或從模型中選擇 / Epsilon-greedy action selection\n",
    "        # if random.random() < epsilon:\n",
    "        #     action = random.choice(env.action_space)\n",
    "        # else:\n",
    "        #     with torch.no_grad():\n",
    "        #         state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        #         action = policy_net(state_tensor).argmax(dim=1).item()\n",
    "        \n",
    "\n",
    "        frame_idx += 1\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(num_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = online_net(cur_state_b.to(device))\n",
    "                action = q.argmax(dim=1).item()  # 取得最大 Q 值對應的動作\n",
    "\n",
    "        # 執行動作 / Perform action\n",
    "        next_state_raw, reward, done, info = env.step(action)\n",
    "\n",
    "        next_state_b, next_state = preprocess_state(next_state_raw, cur_state, False)\n",
    "        # 存入經驗  (去掉 batch 維度)\n",
    "        memory.push(cur_state.cpu(), action, reward, next_state.cpu(), done)\n",
    "        cur_state_b, cur_state = next_state_b, next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # 訓練模型 / Train the model from memory\n",
    "        if len(memory) >= batch_size:\n",
    "            for _ in range(1): # 你可以調整這個數字，例如 2 或 4\n",
    "                states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "                # states = states.to(device)\n",
    "                # next_states = next_states.to(device)\n",
    "                # actions = actions.to(device)\n",
    "                # rewards = rewards.to(device)\n",
    "                # dones = dones.to(device)\n",
    "                states = states.to(device, non_blocking=True).float()\n",
    "                next_states = next_states.to(device, non_blocking=True).float()\n",
    "                actions = actions.to(device, non_blocking=True)\n",
    "                rewards = rewards.to(device, non_blocking=True)\n",
    "                dones = dones.to(device, non_blocking=True)\n",
    "\n",
    "                with torch.amp.autocast(\"cuda\"):\n",
    "                    # 計算 Q 值 / Compute Q values\n",
    "                    q_values = online_net(states)\n",
    "                    q_values = q_values.gather(1, actions)  # 取得當前動作的 Q 值\n",
    "\n",
    "                    # 計算目標 Q 值 / Compute target Q values\n",
    "                    next_actions = online_net(next_state).argmax(1, keepdim=True).unsqueeze(1)\n",
    "                    next_q_values = target_net(next_state).gather(1, next_actions).squeeze(1)\n",
    "\n",
    "                    expected_q = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "                    loss = F.mse_loss(q_values, expected_q)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "\n",
    "        # --- 更新 target_net ---\n",
    "        if frame_idx % target_update_freq == 0: \n",
    "            target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    # 更新 epsilon 並紀錄統計資料 / Decay epsilon and log results\n",
    "    \n",
    "    reward_history.append(episode_reward)\n",
    "    score_history.append(info['score'])\n",
    "    \n",
    "    print(f\"Episode {episode + 1:3d}, Reward: {episode_reward:7.2f}, Score: {info['score']:7.2f}, Epsilon: {epsilon:5.3f}\")\n",
    "\n",
    "    # 儲存模型（若比目前最佳分數更好，或每 50 回合）\n",
    "    if (episode + 1) % 50 == 0 or info['score'] > best_score:\n",
    "        best_score = max(best_score, info['score'])\n",
    "\n",
    "        torch.save({\n",
    "            'online_net': online_net.state_dict(),\n",
    "            'target_net': target_net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epsilon': epsilon,\n",
    "            'episode': episode,\n",
    "            'total_steps': frame_idx,\n",
    "            'best_score': best_score,\n",
    "            'reward_history': reward_history,\n",
    "            'score_history': score_history,\n",
    "        }, 'checkpoint_V10.pth')\n",
    "\n",
    "        print(f\"Checkpoint saved at episode {episode + 1}, best score: {best_score:.2f}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "reward_smooth = gaussian_filter1d(reward_history, sigma=50)\n",
    "score_smooth = gaussian_filter1d(score_history, sigma=50)\n",
    "\n",
    "# 畫圖\n",
    "# plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_history, label='Total Reward', alpha=0.8)\n",
    "plt.plot(score_history, label='Score', alpha=0.8)\n",
    "plt.plot(reward_smooth, label='Smoothed Total Reward', linewidth=2)\n",
    "plt.plot(score_smooth, label='Smoothed Score', linewidth=2)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Training Reward Over Episodes\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"dqn_training_curve.png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
